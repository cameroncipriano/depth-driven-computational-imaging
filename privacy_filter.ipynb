{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EC520 Image Processing and Communication\n",
    "## Privacy Filter\n",
    "Cameron Cipriano, Molly Housego\n",
    "\n",
    "Depth-Driven Computational Imaging: Privacy Filter\n",
    "\n",
    "The goal of the privacy filter is to identify people's faces in a scene and minimally distort them such that a facial recognition system will be unable to determine who it is. To perform minimal facial distortion, RGB and RGB+D Images were captured of a scene:\n",
    "1. Facial Detection finds each face present in the image\n",
    "2. Facial bounding boxes projected into RGB+D images to determine which points are facial pixels\n",
    "3. Facial pixel coordinates reprojected to 2D image to define outline of face\n",
    "4. Blur is applied to facial region and non-facial pixels are re-inserted to only distort the face.\n",
    "\n",
    "Facial Detection implemented using a Haar Cascade Classifier\n",
    "\n",
    "Facial recognition was implemented using a covariance matrix approach with Nearest Neighbor matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple\n",
    "from glob import glob\n",
    "import csv\n",
    "from itertools import product\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pptk\n",
    "from scipy.linalg import eigh, svd\n",
    "from tqdm.notebook import tqdm\n",
    "import alphashape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_directories(bbox_path, blur_path):\n",
    "    bbox_image_names = glob(os.path.join(bbox_path, '*.jpg'))\n",
    "    blur_image_names = glob(os.path.join(blur_path, '*.jpg'))\n",
    "\n",
    "    for (bbox_img, blur_img) in zip(bbox_image_names, blur_image_names):\n",
    "        try:\n",
    "            os.remove(bbox_img)\n",
    "            os.remove(blur_img)\n",
    "        except OSError as e:\n",
    "            print(f'Error: {e.strerror}')\n",
    "\n",
    "def load_point_clouds(path):\n",
    "    pointcloud_filenames = sorted(glob(os.path.join(path, '*.csv')))\n",
    "    pointclouds = {}\n",
    "    # Setup Progress bar\n",
    "    iterations = tqdm(pointcloud_filenames, unit='Files')\n",
    "    for file_idx, csvfile in enumerate(iterations):\n",
    "        # each point cloud is 39,963 points, each with xyz and rgb values\n",
    "        pointclouds[file_idx] = {'points': np.zeros((39963,3)), 'colors': np.zeros((39963,3))}\n",
    "        with open(csvfile, newline='') as pointcloud_file:\n",
    "            iterations.set_description(f\"Parsing: '{pointcloud_file.name}'\")\n",
    "            point_reader = csv.reader(pointcloud_file)\n",
    "            for point_idx, point_vals in enumerate(point_reader):\n",
    "                # xyx for current point cloud\n",
    "                pointclouds[file_idx]['points'][point_idx, 0] = point_vals[0]\n",
    "                pointclouds[file_idx]['points'][point_idx, 1] = point_vals[1]\n",
    "                pointclouds[file_idx]['points'][point_idx, 2] = point_vals[2]\n",
    "\n",
    "                # RGB for current point cloud\n",
    "                pointclouds[file_idx]['colors'][point_idx, 0] = point_vals[3]\n",
    "                pointclouds[file_idx]['colors'][point_idx, 1] = point_vals[4]\n",
    "                pointclouds[file_idx]['colors'][point_idx, 2] = point_vals[5]\n",
    "    \n",
    "    return pointclouds\n",
    "\n",
    "def load_images(path):\n",
    "    # Read Images containing faces\n",
    "    image_filenames = sorted(glob(os.path.join(path, '*.jpg')))\n",
    "    \n",
    "    # Setup Progress bar\n",
    "    image_iterations = tqdm(image_filenames, unit='Images')\n",
    "    \n",
    "    images = []\n",
    "    for filename in image_iterations:\n",
    "        image_iterations.set_description(f\"Loading: '{filename}'\")\n",
    "        raw_img = cv.imread(filename)\n",
    "\n",
    "        img_scale = 512 / raw_img.shape[0]\n",
    "        scaled_width  = round(img_scale * raw_img.shape[1])\n",
    "        scaled_height = round(img_scale * raw_img.shape[0])\n",
    "\n",
    "        images.append(cv.resize(raw_img, (scaled_width, scaled_height), interpolation=cv.INTER_AREA))\n",
    "    \n",
    "    return np.array(images)\n",
    "\n",
    "def display_images(images):\n",
    "    for image in images:\n",
    "        cv.imshow('Img', image)\n",
    "        cv.waitKey(0)\n",
    "    \n",
    "    cv.destroyAllWindows()\n",
    "    cv.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facial Recognition - Implementation of Region Covariance Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distance(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    Vx, Dx, V_Tx = svd(x)\n",
    "    Vy, Dy, V_Ty = svd(y)\n",
    "\n",
    "    Dx_log = np.log10(Dx)\n",
    "    Dy_log = np.log10(Dy)\n",
    "\n",
    "    log_C_x = Vx @ np.diag(Dx_log) @ V_Tx\n",
    "    log_C_y = Vy @ np.diag(Dy_log) @ V_Ty\n",
    "\n",
    "    return np.linalg.norm((log_C_x - log_C_y))\n",
    "\n",
    "class RegionCovarianceDetector:\n",
    "    \"\"\"\n",
    "    Object Detector Using Region Covariance\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    coord : bool, optional (default=True)\n",
    "        Whether use coordinates as features or not.\n",
    "\n",
    "    color : bool, optional (default=True)\n",
    "        Whether use color channels as features or not.\n",
    "\n",
    "    intensity : bool, optional (default=False)\n",
    "        Whether use intensity as feature or not.\n",
    "\n",
    "    first_order : bool, optional (default=True)\n",
    "        Scharr Filter applied to intensity image, first order derivative. If False, no filters are used.\n",
    "\n",
    "    second_order : bool, optional (default=True)\n",
    "        Scharr Filter applied to intensity image, second order derivative. If False, no filters are used.\n",
    "\n",
    "    ratio : float, optional (default=1.15)\n",
    "        Scaling factor between two consecutive scales of the search window size and step size.\n",
    "\n",
    "    step : int, optional (default=3)\n",
    "        The minimum step size.\n",
    "\n",
    "    n_scales : int, optional (default=9)\n",
    "        The number of scales of the windows.\n",
    "\n",
    "    eps : float, optional (default=1e-16)\n",
    "        Small number to keep covariance matrices in SPD.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    object_shape : (int, int)\n",
    "        The object's shape.\n",
    "\n",
    "    object_covariance : np.ndarray, shape (n_features, n_features)\n",
    "        Covariance matrix of the object.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            coord: bool = True,\n",
    "            color: bool = True,\n",
    "            intensity: bool = False,\n",
    "            first_order: bool = True,\n",
    "            second_order: bool = True,\n",
    "            ratio: float = 1.15,\n",
    "            step: int = 3,\n",
    "            n_scales: int = 9,\n",
    "            eps: float = 1e-16\n",
    "    ):\n",
    "        self.coord = coord\n",
    "        self.color = color\n",
    "        self.intensity = intensity\n",
    "        self.first_order = first_order\n",
    "        self.second_order = second_order\n",
    "        self.ratio = ratio\n",
    "        self.step = step\n",
    "        self.n_scales = n_scales\n",
    "        self.eps = eps\n",
    "\n",
    "    def extract_features(self, img: np.ndarray) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Extract image features.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img : np.ndarray, shape (h, w, c)\n",
    "            uint8 RGB image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        features : a list of np.ndarray\n",
    "            Features such as intensity, its gradient and so on.\n",
    "        \"\"\"\n",
    "        h, w, c = img.shape[:3]\n",
    "        intensity = cv.cvtColor(img, cv.COLOR_BGR2HSV)[:, :, 2] / 255.\n",
    "        features = list()\n",
    "\n",
    "        # use coordinates\n",
    "        if self.coord:\n",
    "            features.append(np.tile(np.arange(w, dtype=float), (h, 1)))\n",
    "            features.append(np.tile(np.arange(h, dtype=float).reshape(-1, 1), (1, w)))\n",
    "\n",
    "        # use color channels\n",
    "        if self.color:\n",
    "            for i in range(c):\n",
    "                features.append(img[:, :, i].astype(float) / 255.)\n",
    "\n",
    "        # use intensity\n",
    "        if self.intensity:\n",
    "            features.append(intensity)\n",
    "\n",
    "        # use 1st-order derivatives of x and y of intensity image\n",
    "        if self.first_order:\n",
    "            first_order_x = np.abs(cv.Scharr(intensity, cv.CV_16S, 1, 0))\n",
    "            first_order_y = np.abs(cv.Scharr(intensity, cv.CV_16S, 0, 1))\n",
    "            features.extend([first_order_x, first_order_y])\n",
    "\n",
    "        # use 2nd-order derivatives of x and y of intensity image\n",
    "        if self.second_order and self.first_order:\n",
    "            features.append(np.abs(cv.Scharr(first_order_x, cv.CV_16S, 1, 0)))\n",
    "            features.append(np.abs(cv.Scharr(first_order_y, cv.CV_16S, 0, 1)))\n",
    "\n",
    "        return features\n",
    "\n",
    "    def calc_integral_images(self, img: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Calculate integral images.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img : np.ndarray, shape (h, w, c)\n",
    "            uint8 RGB image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        P : np.ndarray, shape (h+1, w+1, n_features)\n",
    "            First order integral images of features.\n",
    "\n",
    "        Q : np.ndarray, shape (h+1, w+1, n_features, n_features)\n",
    "            Second order integral images of features.\n",
    "        \"\"\"\n",
    "        h, w = img.shape[:2]\n",
    "        features = self.extract_features(img)\n",
    "        length = len(features)\n",
    "\n",
    "        # first order integral images\n",
    "        P = cv.integral(np.array(features).transpose((1, 2, 0)))\n",
    "\n",
    "        # second order integral images\n",
    "        Q = cv.integral(\n",
    "            np.array(list(map(lambda x: x[0] * x[1], product(features, features)))).transpose((1, 2, 0))\n",
    "        )\n",
    "        Q = Q.reshape(h + 1, w + 1, length, length)\n",
    "        return P, Q\n",
    "\n",
    "    def calc_covariance(self, P: np.ndarray, Q: np.ndarray, pt1: Tuple[int, int], pt2: Tuple[int, int]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate covariance matrix from integral images.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        P : np.ndarray, shape (h+1, w+1, n_features)\n",
    "            First order integral images of features.\n",
    "\n",
    "        Q : np.ndarray, shape (h+1, w+1, n_features, n_features)\n",
    "            Second order integral images of features.\n",
    "\n",
    "        pt1 : (int, int)\n",
    "            Left top coordinate.\n",
    "\n",
    "        pt2 : (int, int)\n",
    "            Right bottom coordinate.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        covariance : np.ndarray, shape (n_features, n_features)\n",
    "            Covariance matrix.\n",
    "        \"\"\"\n",
    "        x1, y1 = pt1\n",
    "        x2, y2 = pt2\n",
    "        q = Q[y2, x2] + Q[y1, x1] - Q[y1, x2] - Q[y2, x1]\n",
    "        p = P[y2, x2] + P[y1, x1] - P[y1, x2] - P[y2, x1]\n",
    "        n = (y2 - y1) * (x2 - x1)\n",
    "        covariance = np.abs((q - np.outer(p, p) / n) / (n - 1)) + (self.eps * np.identity(P.shape[2]))\n",
    "        return covariance\n",
    "\n",
    "    def set_search_object(self, img: np.ndarray) -> Tuple[np.ndarray, Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Calculate the object covariance matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img : np.ndarray, shape (h, w, c)\n",
    "            uint8 RGB image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        : Fitted detector.\n",
    "        \"\"\"\n",
    "        h, w = img.shape[:2]\n",
    "        P, Q = self.calc_integral_images(img)\n",
    "\n",
    "        # normalize about coordinates\n",
    "        if self.coord:\n",
    "            for i, size in enumerate((w, h)):\n",
    "                P[:, :, i] /= size\n",
    "                Q[:, :, i] /= size\n",
    "                Q[:, :, :, i] /= size\n",
    "\n",
    "        # calculate covariance matrix\n",
    "        obj_cov = self.calc_covariance(P, Q, (0, 0), (w, h))\n",
    "        obj_shape = (h, w)\n",
    "\n",
    "        return obj_cov, obj_shape\n",
    "\n",
    "    def find_object(self, object: Tuple[np.ndarray, Tuple[int, int]], target_img: np.ndarray) -> Tuple[Tuple[int, int], Tuple[int, int], float]:\n",
    "        \"\"\"\n",
    "        Compute object's position in the target image.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        object : (np.ndarray, (h, w))\n",
    "            Object's representative covariance matrix and its height and width \n",
    "\n",
    "        target_img : np.ndarray, shape (h, w, c)\n",
    "            uint8 RGB image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pt1 : (int, int)\n",
    "            Left top coordinate.\n",
    "\n",
    "        pt2 : (int, int)\n",
    "            Right bottom coordinate.\n",
    "\n",
    "        score : float\n",
    "            Dissimilarity of object and target covariance matrices.\n",
    "        \"\"\"\n",
    "        target_h, target_w = target_img.shape[:2]\n",
    "        obj_h, obj_w = object[1]\n",
    "        P, Q = self.calc_integral_images(target_img)\n",
    "\n",
    "        # search window's shape and step size\n",
    "        end = (self.n_scales + 1) // 2\n",
    "        start = end - self.n_scales\n",
    "        shapes = [(int(obj_h * self.ratio ** i), int(obj_w * self.ratio ** i)) for i in range(start, end)]\n",
    "        steps = [int(self.step * self.ratio ** i) for i in range(self.n_scales)]\n",
    "\n",
    "        level_iterations = tqdm(zip(shapes, steps), total=len(shapes))\n",
    "        level_iterations\n",
    "        distances = list()\n",
    "        for level, (shape, step) in enumerate(level_iterations):\n",
    "            level_iterations.set_description(f'Level: {level}, Shape: {shape}, Step: {step}')\n",
    "            p_h, p_w = shape\n",
    "            p_P, p_Q = P.copy(), Q.copy()\n",
    "\n",
    "            # normalize about coordinates\n",
    "            if self.coord:\n",
    "                for i, size in enumerate((p_w, p_h)):\n",
    "                    p_P[:, :, i] /= size\n",
    "                    p_Q[:, :, i] /= size\n",
    "                    p_Q[:, :, :, i] /= size\n",
    "\n",
    "            distance = list()\n",
    "            y1, y2 = 0, p_h\n",
    "            while y2 <= target_h:\n",
    "                dist = list()\n",
    "                x1, x2 = 0, p_w\n",
    "                while x2 <= target_w:\n",
    "                    # calculate covariance matrix\n",
    "                    p_cov = self.calc_covariance(p_P, p_Q, (x1, y1), (x2, y2))\n",
    "\n",
    "                    # jump horizontally\n",
    "                    x1 += step\n",
    "                    x2 += step\n",
    "\n",
    "                    # calculate dissimilarity of two covariance matrices\n",
    "                    dist.append(calc_distance(object[0], p_cov))\n",
    "\n",
    "                # jump vertically\n",
    "                y1 += step\n",
    "                y2 += step\n",
    "                distance.append(dist)\n",
    "            distances.append(np.array(distance))\n",
    "\n",
    "        # choose the most similar window\n",
    "        min_dist_indices = list(map(np.argmin, distances))\n",
    "        min_dist_index = int(np.argmin([dist.flatten()[i] for i, dist in zip(min_dist_indices, distances)]))\n",
    "        min_step = steps[min_dist_index]\n",
    "        min_shape = shapes[min_dist_index]\n",
    "        min_index = min_dist_indices[min_dist_index]\n",
    "        b_h, b_w = distances[min_dist_index].shape\n",
    "\n",
    "        pt1 = ((min_index % b_w) * min_step, (min_index // b_w) * min_step)\n",
    "        pt2 = (pt1[0] + min_shape[1], pt1[1] + min_shape[0])\n",
    "        score = distances[min_dist_index].flatten()[min_index]\n",
    "        \n",
    "        return pt1, pt2, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_camera(path):\n",
    "    CHECKERBOARD = (6, 9)\n",
    "    sub_pix_criteria = (cv.TERM_CRITERIA_EPS + cv.TermCriteria_MAX_ITER, 30,\n",
    "                        0.01)\n",
    "\n",
    "    # Creating vector to store vectors of 3D points for each chessboard\n",
    "    obj_points = []\n",
    "\n",
    "    # Creating vector to store vectors of 2D points for each chessboard\n",
    "    img_points = []\n",
    "\n",
    "    # Defining the world coordinates for 3D poitns\n",
    "    obj_p = np.zeros((1, CHECKERBOARD[0] * CHECKERBOARD[1], 3), np.float32)\n",
    "    obj_p[0, :, :2] = np.mgrid[0:CHECKERBOARD[0],\n",
    "                                0:CHECKERBOARD[1]].T.reshape(-1, 2)\n",
    "\n",
    "    # Read Images containing faces\n",
    "    image_filenames = sorted(glob(os.path.join(path, '*.jpg')))\n",
    "    \n",
    "    # Setup Progress bar\n",
    "    image_iterations = tqdm(image_filenames, unit='Images')\n",
    "    for filename in image_iterations:\n",
    "        image_iterations.set_description('Reading calibration image')\n",
    "        img = cv.imread(filename)\n",
    "        gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Find the chessboard corners.\n",
    "        # Ret will be true if the entire pattern is successfully found\n",
    "        image_iterations.set_description('Finding chessboard corners')\n",
    "        ret, corners = cv.findChessboardCorners(gray, CHECKERBOARD)\n",
    "\n",
    "        if ret == True:\n",
    "            image_iterations.set_description('Refining corners')\n",
    "            obj_points.append(obj_p)\n",
    "            corners_2 = cv.cornerSubPix(gray, corners, (11, 11), (-1, -1),\n",
    "                                            sub_pix_criteria)\n",
    "\n",
    "            img = cv.drawChessboardCorners(img, CHECKERBOARD, corners_2, ret)\n",
    "            cv.imshow('img', img)\n",
    "            cv.waitKey(0)\n",
    "\n",
    "            image_iterations.set_description('Defining image points')\n",
    "            img_points.append(corners_2)\n",
    "        else:\n",
    "            print(\n",
    "                f\"COULD NOT FIND CORNERS! Please take a replacement photo for {filename}\"\n",
    "            )\n",
    "\n",
    "    cv.destroyAllWindows()\n",
    "    cv.waitKey(1)\n",
    "    rms, intrinsic_matrix, dist_coeffs, rotation_vecs, translation_vecs = cv.calibrateCamera(\n",
    "        obj_points, img_points, gray.shape[::-1], None, None)\n",
    "\n",
    "    f_x = intrinsic_matrix[0, 0]\n",
    "    f_y = intrinsic_matrix[1, 1]\n",
    "    pp_x = intrinsic_matrix[0, 2]\n",
    "    pp_y = intrinsic_matrix[1, 2]\n",
    "\n",
    "    inv_intrinsic = np.array([[1/f_x, 0, -pp_x/f_x],\n",
    "                              [0, 1/f_y, -pp_y/f_y],\n",
    "                              [0,     0,        1]])\n",
    "\n",
    "    print(f'Camera calibration RMS Error: {rms}')\n",
    "    return intrinsic_matrix, inv_intrinsic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Algorithm Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces(images, classifier):\n",
    "    \"\"\"\n",
    "    Function for detecting faces\n",
    "\n",
    "    Returns list of rectangles\n",
    "    \"\"\"\n",
    "    detection_iterations = tqdm(images, unit='Image')\n",
    "\n",
    "    face_bboxes = {}\n",
    "    detected_face_visuals = []\n",
    "    for img_idx, frame in enumerate(detection_iterations):\n",
    "        detection_iterations.set_description('Detecting faces')\n",
    "        frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Histogram equalization to improve contrast and make grayscale image more uniform\n",
    "        frame_gray = cv.equalizeHist(frame_gray)\n",
    "\n",
    "        #-- Detect faces\n",
    "        # faces = classifier.detectMultiScale(frame_gray, scaleFactor=1.01, minNeighbors=7, minSize=(175, 175), maxSize=(300, 300), flags=cv.CASCADE_SCALE_IMAGE)\n",
    "        faces = classifier.detectMultiScale(frame_gray, scaleFactor=1.05, minNeighbors=6, minSize=(30, 30), maxSize=(75, 75), flags=cv.CASCADE_SCALE_IMAGE)\n",
    "        visual = frame.copy()\n",
    "        for (x,y,w,h) in faces:\n",
    "            # COLOR IS BGR!\n",
    "            visual = cv.rectangle(visual, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "            cv.imshow('training  image', frame[y:y+h, x:x+w])\n",
    "            cv.waitKey(0)\n",
    "            \n",
    "        cv.destroyAllWindows()\n",
    "        cv.waitKey(1)\n",
    "        face_bboxes[img_idx] = faces\n",
    "        detected_face_visuals.append(visual)\n",
    "\n",
    "    return np.array(detected_face_visuals), face_bboxes\n",
    "\n",
    "def recognize_faces(images, face_database, face_bboxes, recognizer: RegionCovarianceDetector):\n",
    "    recognition_visuals = []\n",
    "    img_labels = {}\n",
    "    for (img_idx, boxes) in face_bboxes.items():\n",
    "        person_labels = []\n",
    "        visual = images[img_idx].copy()\n",
    "        for (x, y, w, h) in boxes:\n",
    "            # Grab detected face in the image\n",
    "            detected_face = images[img_idx][y:y+h, x:x+w, :]\n",
    "            # Create covariance matrix of image\n",
    "            img_face_cov, _ = recognizer.set_search_object(detected_face)\n",
    "\n",
    "            distances = [calc_distance(face_obj[0], img_face_cov) for face_obj in face_database]\n",
    "            \n",
    "            person_number = np.argmin(distances)\n",
    "            person_labels.append(person_number)\n",
    "\n",
    "            visual = cv.rectangle(visual, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "            visual = cv.putText(visual, f'Person {person_number}', (x+w, y+h), cv.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255), 1)\n",
    "            recognition_visuals.append(visual)\n",
    "\n",
    "        img_labels[img_idx] = person_labels\n",
    "    \n",
    "    return np.array(recognition_visuals), img_labels\n",
    "\n",
    "\n",
    "def recover_face_outlines(images, bboxes, pointclouds):\n",
    "    # Images and bboxes would have been \n",
    "    face_outlines = {}\n",
    "    for (img_idx, pointcloud) in pointclouds.items():\n",
    "        # Create viewer of the pointcloud\n",
    "        viewer = pptk.viewer(pointcloud['points'], pointcloud['colors'] / 255)\n",
    "        # Make sure we are looking head on from where the iPad was setup\n",
    "        viewer.set(lookat=np.zeros((3, 1)), phi=-np.pi/2, theta=0, point_size=2)\n",
    "        print('Select the area where a face exists')\n",
    "        viewer.wait()\n",
    "\n",
    "        face_indices = viewer.get('selected')\n",
    "\n",
    "        if face_indices.shape[0] == 0:\n",
    "            viewer.close()\n",
    "            continue\n",
    "\n",
    "        raw_points = pointcloud['points'][face_indices]\n",
    "\n",
    "        # Threshold the distances in this area\n",
    "        depths = np.histogram_bin_edges(raw_points[:, 1], 5)\n",
    "        indices_to_keep = np.where(raw_points[:, 1] <= depths[1])[0]\n",
    "        face_points = raw_points[indices_to_keep]\n",
    "\n",
    "        # Filter the point cloud selection to find only the face pixels\n",
    "        filtered_selection = face_indices[indices_to_keep]\n",
    "        viewer.set(selected=filtered_selection)\n",
    "\n",
    "        # Set the indicated face_points to the output\n",
    "        # Must swap the y and z columns because y is actually depth from the iPad\n",
    "        face_outlines[img_idx] = face_points[:, [0, 2, 1]]\n",
    "        viewer.wait()\n",
    "        \n",
    "        viewer.close()\n",
    "    \n",
    "    return face_outlines    \n",
    "\n",
    "def blur_face_outlines(images, face_bboxes, outlines, K, K_INV):\n",
    "    for (img_idx, outline_pts_3d) in outlines.items():\n",
    "        outline_pts_2d = K @ outline_pts_3d.T\n",
    "        z_coords = outline_pts_2d[-1]\n",
    "        # get pixel coords\n",
    "        outline_pts_2d = np.round(outline_pts_2d[:2] / z_coords).T\n",
    "\n",
    "        # print(outline_pts_2d)\n",
    "        face_shape = alphashape.alphashape(outline_pts_2d, 0.05)\n",
    "        mask = np.zeros((3024, 4032))\n",
    "\n",
    "        x_coords, y_coords = face_shape.exterior.coords.xy\n",
    "        x_coords = np.array(x_coords, dtype=np.int32).reshape(-1, 1)\n",
    "        y_coords = np.array(y_coords, dtype=np.int32).reshape(-1, 1)\n",
    "\n",
    "\n",
    "        coords = np.hstack((y_coords, x_coords))\n",
    "        mask = cv.fillPoly(mask, [coords], (255, 255, 255))\n",
    "        # mask = cv.polylines(mask, [coords], True, (255, 255, 255))\n",
    "\n",
    "        cv.imshow('mask', mask)\n",
    "        cv.waitKey(0)\n",
    "    \n",
    "    cv.destroyAllWindows()\n",
    "    cv.waitKey(1)\n",
    "\n",
    "\n",
    "\n",
    "def generate_facial_recognition_database(path: str, face_detector, face_recognizer: RegionCovarianceDetector) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    image_filenames = sorted(glob(os.path.join(path, '*_frontal.jpg')))\n",
    "    image_iterations = tqdm(image_filenames, unit='Images')\n",
    "    \n",
    "    images = []\n",
    "    for filename in image_iterations:\n",
    "        image_iterations.set_description(f'Loading templating images...')\n",
    "        \n",
    "        raw_img = cv.imread(filename)\n",
    "\n",
    "        img_scale = 512 / raw_img.shape[0]\n",
    "        scaled_width  = round(img_scale * raw_img.shape[1])\n",
    "        scaled_height = round(img_scale * raw_img.shape[0])\n",
    "\n",
    "        images.append(cv.resize(raw_img, (scaled_width, scaled_height), interpolation=cv.INTER_AREA))\n",
    "    \n",
    "    images = np.array(images)\n",
    "    \n",
    "    # Extract face templates to be used for facial recognition\n",
    "    face_templates = []\n",
    "    _, face_bboxes = detect_faces(images, face_detector)\n",
    "    for (img_idx, boxes) in face_bboxes.items():\n",
    "        for (x, y, w, h) in boxes:\n",
    "            face_templates.append(images[img_idx][y:y+h, x:x+w, :])\n",
    "    \n",
    "    template_iterations = tqdm(face_templates, unit='Templates')\n",
    "    template_iterations.set_description('Defining face templates')\n",
    "    database = [face_recognizer.set_search_object(template) for template in template_iterations]\n",
    "\n",
    "    return database\n",
    "\n",
    "def blur_and_detect_faces(images, face_bboxes, face_database, detector, recognizer, blur_sigmas):\n",
    "    images_with_blurs = {}\n",
    "    for sigma in blur_sigmas:\n",
    "        detection_post_blur = {'detections': [], 'recognitions': [], 'labels': []}\n",
    "        for (img_idx, boxes) in face_bboxes.items():\n",
    "            visual = images[img_idx].copy()\n",
    "            for (x, y, w, h) in boxes:\n",
    "                ROI = visual[y:y+h, x:x+w]\n",
    "                blur = cv.GaussianBlur(ROI, (0, 0), sigma)\n",
    "                visual[y:y+h,x:x+w] = blur\n",
    "                detection_img, detection_box = detect_faces([visual], detector)\n",
    "                for bbox in detection_box.values():\n",
    "                    if len(bbox) > 0:\n",
    "                        detection_post_blur['detections'].append(detection_img[0])\n",
    "                        recog_img, identifier = recognize_faces([detection_img[0]], face_database, detection_box, recognizer)\n",
    "                        recog_img[0] = cv.putText(recog_img[0], f'Sigma: {sigma}', (50,50), cv.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)\n",
    "                        detection_post_blur['recognitions'].append(recog_img[0])\n",
    "                        for person_class in identifier.values():\n",
    "                            print(f'Detected person: {person_class}')\n",
    "                            detection_post_blur['labels'].append(person_class)\n",
    "                    else:\n",
    "                        print('No face detected')\n",
    "            \n",
    "        detection_post_blur['detections'] = np.array(detection_post_blur['detections'])\n",
    "        detection_post_blur['recognitions'] = np.array(detection_post_blur['recognitions'])\n",
    "        images_with_blurs[sigma] = detection_post_blur\n",
    "\n",
    "    return images_with_blurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img_path = 'images/jpg'\n",
    "blurred_img_path = 'images/jpg/blurred'\n",
    "bbox_img_path = 'images/jpg/bounding_boxes'\n",
    "calibration_img_path = 'images/jpg/calibration'\n",
    "pointcloud_path = 'pointclouds'\n",
    "\n",
    "\n",
    "# Make sure output directories are empty\n",
    "clean_directories(bbox_img_path, blurred_img_path)\n",
    "\n",
    "# Load in the Facial Detection classifiers\n",
    "face_cascade_alt2 = cv.CascadeClassifier('cascades/haarcascade_frontalface_alt2.xml')\n",
    "\n",
    "# Create Facial Recognizer\n",
    "face_recognizer = RegionCovarianceDetector()\n",
    "\n",
    "print('Generating Facial Recognition Database')\n",
    "# database                       = generate_facial_recognition_database(input_img_path, face_cascade_alt2, face_recognizer)\n",
    "print('Loading in images of people')\n",
    "images                         = load_images(input_img_path)\n",
    "\n",
    "print('Loading in pointclouds')\n",
    "# pointclouds                    = load_point_clouds(pointcloud_path)\n",
    "print('Calibrating camera using chessboard patterns')\n",
    "K_camera, inv_K_camera         = calibrate_camera(calibration_img_path)\n",
    "print('Detecting faces in images')\n",
    "# images_with_faces, face_bboxes = detect_faces(images, face_cascade_alt2)\n",
    "# display_images(images_with_faces)\n",
    "print('Recovering facial outline based on pointclouds')\n",
    "# images_with_id, identifiers    = recognize_faces(images, database, face_bboxes, face_recognizer)\n",
    "# display_images(images_with_id)\n",
    "# face_outlines                  = recover_face_outlines(images, face_bboxes, pointclouds)\n",
    "\n",
    "print('Blurring faces and running recognition')\n",
    "# detections_after_blur          = blur_and_detect_faces(images, face_bboxes, database, face_cascade_alt2, face_recognizer, range(1, 5))\n",
    "\n",
    "# for (img_idx, details) in detections_after_blur.items():\n",
    "    # display_images(details['recognitions'])\n",
    "\n",
    "\n",
    "# display_images(detections_after_blur[1]['detections'])\n",
    "\n",
    "# blurred_images                 = blur_face_outlines(images, face_bboxes, face_outlines, K_camera, inv_K_camera)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1373ee5c41634514af23980f60d0958fc0f2d6819915eababf22cdf09658028c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('ec520_final_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
