{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EC520 Image Processing and Communication\n",
    "## Privacy Filter\n",
    "Cameron Cipriano, Molly Housego\n",
    "\n",
    "Depth-Driven Computational Imaging: Privacy Filter\n",
    "\n",
    "The goal of the privacy filter is to identify people's faces in a scene and minimally distort them such that a facial recognition system will be unable to determine who it is. To perform minimal facial distortion, RGB and RGB+D Images were captured of a scene:\n",
    "1. Facial Detection finds each face present in the image\n",
    "2. Facial bounding boxes projected into RGB+D images to determine which points are facial pixels\n",
    "3. Facial pixel coordinates reprojected to 2D image to define outline of face\n",
    "4. Blur is applied to facial region and non-facial pixels are re-inserted to only distort the face.\n",
    "\n",
    "Facial Detection implemented using a Haar Cascade Classifier\n",
    "\n",
    "Facial recognition was implemented using a covariance matrix approach with Nearest Neighbor matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple\n",
    "from glob import glob\n",
    "import csv\n",
    "from itertools import product\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pptk\n",
    "from scipy.linalg import eigh, svd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDirectories(bbox_path, blur_path):\n",
    "    bbox_image_names = glob(os.path.join(bbox_path, '*.jpg'))\n",
    "    blur_image_names = glob(os.path.join(blur_path, '*.jpg'))\n",
    "\n",
    "    for (bbox_img, blur_img) in zip(bbox_image_names, blur_image_names):\n",
    "        try:\n",
    "            os.remove(bbox_img)\n",
    "            os.remove(blur_img)\n",
    "        except OSError as e:\n",
    "            print(f'Error: {e.strerror}')\n",
    "\n",
    "def loadPointClouds(path):\n",
    "    pointcloud_filenames = sorted(glob(os.path.join(path, '*.csv')))\n",
    "    pointclouds = {}\n",
    "    # Setup Progress bar\n",
    "    iterations = tqdm(pointcloud_filenames, unit='Files')\n",
    "    for file_idx, csvfile in enumerate(iterations):\n",
    "        # each point cloud is 39,963 points, each with xyz and rgb values\n",
    "        pointclouds[file_idx] = {'points': np.zeros((39963,3)), 'colors': np.zeros((39963,3))}\n",
    "        with open(csvfile, newline='') as pointcloud_file:\n",
    "            iterations.set_description(f\"Parsing: '{pointcloud_file.name}'\")\n",
    "            point_reader = csv.reader(pointcloud_file)\n",
    "            for point_idx, point_vals in enumerate(point_reader):\n",
    "                # xyx for current point cloud\n",
    "                pointclouds[file_idx]['points'][point_idx, 0] = point_vals[0]\n",
    "                pointclouds[file_idx]['points'][point_idx, 1] = point_vals[1]\n",
    "                pointclouds[file_idx]['points'][point_idx, 2] = point_vals[2]\n",
    "\n",
    "                # RGB for current point cloud\n",
    "                pointclouds[file_idx]['colors'][point_idx, 0] = point_vals[3]\n",
    "                pointclouds[file_idx]['colors'][point_idx, 1] = point_vals[4]\n",
    "                pointclouds[file_idx]['colors'][point_idx, 2] = point_vals[5]\n",
    "    \n",
    "    return pointclouds\n",
    "\n",
    "def loadImages(path):\n",
    "    # Read Images containing faces\n",
    "    image_filenames = sorted(glob(os.path.join(path, '*.jpg')))\n",
    "    \n",
    "    # Setup Progress bar\n",
    "    image_iterations = tqdm(image_filenames, unit='Images')\n",
    "    \n",
    "    images = []\n",
    "    for filename in image_iterations:\n",
    "        image_iterations.set_description(f\"Loading: '{filename}'\")\n",
    "        raw_img = cv.imread(filename)\n",
    "\n",
    "        img_scale = 512 / raw_img.shape[0]\n",
    "        scaled_width  = int(img_scale * raw_img.shape[1])\n",
    "        scaled_height = int(img_scale * raw_img.shape[0])\n",
    "\n",
    "        images.append(cv.resize(raw_img, (scaled_width, scaled_height), interpolation=cv.INTER_AREA))\n",
    "    \n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facial Recognition - Implementation of Region Covariance Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distance(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    Vx, Dx, V_Tx = svd(x)\n",
    "    Vy, Dy, V_Ty = svd(y)\n",
    "\n",
    "    Dx_log = np.log10(Dx)\n",
    "    Dy_log = np.log10(Dy)\n",
    "\n",
    "    log_C_x = Vx @ np.diag(Dx_log) @ V_Tx\n",
    "    log_C_y = Vy @ np.diag(Dy_log) @ V_Ty\n",
    "\n",
    "    return np.linalg.norm((log_C_x - log_C_y))\n",
    "\n",
    "class RegionCovarianceDetector:\n",
    "    \"\"\"\n",
    "    Object Detector Using Region Covariance\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    coord : bool, optional (default=True)\n",
    "        Whether use coordinates as features or not.\n",
    "\n",
    "    color : bool, optional (default=True)\n",
    "        Whether use color channels as features or not.\n",
    "\n",
    "    intensity : bool, optional (default=False)\n",
    "        Whether use intensity as feature or not.\n",
    "\n",
    "    first_order : bool, optional (default=True)\n",
    "        Scharr Filter applied to intensity image, first order derivative. If False, no filters are used.\n",
    "\n",
    "    second_order : bool, optional (default=True)\n",
    "        Scharr Filter applied to intensity image, second order derivative. If False, no filters are used.\n",
    "\n",
    "    ratio : float, optional (default=1.15)\n",
    "        Scaling factor between two consecutive scales of the search window size and step size.\n",
    "\n",
    "    step : int, optional (default=3)\n",
    "        The minimum step size.\n",
    "\n",
    "    n_scales : int, optional (default=9)\n",
    "        The number of scales of the windows.\n",
    "\n",
    "    eps : float, optional (default=1e-16)\n",
    "        Small number to keep covariance matrices in SPD.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    object_shape : (int, int)\n",
    "        The object's shape.\n",
    "\n",
    "    object_covariance : np.ndarray, shape (n_features, n_features)\n",
    "        Covariance matrix of the object.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            coord: bool = True,\n",
    "            color: bool = True,\n",
    "            intensity: bool = False,\n",
    "            first_order: bool = True,\n",
    "            second_order: bool = True,\n",
    "            ratio: float = 1.15,\n",
    "            step: int = 3,\n",
    "            n_scales: int = 9,\n",
    "            eps: float = 1e-16\n",
    "    ):\n",
    "        self.coord = coord\n",
    "        self.color = color\n",
    "        self.intensity = intensity\n",
    "        self.first_order = first_order\n",
    "        self.second_order = second_order\n",
    "        self.ratio = ratio\n",
    "        self.step = step\n",
    "        self.n_scales = n_scales\n",
    "        self.eps = eps\n",
    "\n",
    "    def extract_features(self, img: np.ndarray) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Extract image features.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img : np.ndarray, shape (h, w, c)\n",
    "            uint8 RGB image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        features : a list of np.ndarray\n",
    "            Features such as intensity, its gradient and so on.\n",
    "        \"\"\"\n",
    "        h, w, c = img.shape[:3]\n",
    "        intensity = cv.cvtColor(img, cv.COLOR_BGR2HSV)[:, :, 2] / 255.\n",
    "        features = list()\n",
    "\n",
    "        # use coordinates\n",
    "        if self.coord:\n",
    "            features.append(np.tile(np.arange(w, dtype=float), (h, 1)))\n",
    "            features.append(np.tile(np.arange(h, dtype=float).reshape(-1, 1), (1, w)))\n",
    "\n",
    "        # use color channels\n",
    "        if self.color:\n",
    "            for i in range(c):\n",
    "                features.append(img[:, :, i].astype(float) / 255.)\n",
    "\n",
    "        # use intensity\n",
    "        if self.intensity:\n",
    "            features.append(intensity)\n",
    "\n",
    "        # use 1st-order derivatives of x and y of intensity image\n",
    "        if self.first_order:\n",
    "            first_order_x = np.abs(cv.Scharr(intensity, cv.CV_16S, 1, 0))\n",
    "            first_order_y = np.abs(cv.Scharr(intensity, cv.CV_16S, 0, 1))\n",
    "            features.extend([first_order_x, first_order_y])\n",
    "\n",
    "        # use 2nd-order derivatives of x and y of intensity image\n",
    "        if self.second_order and self.first_order:\n",
    "            features.append(np.abs(cv.Scharr(first_order_x, cv.CV_16S, 1, 0)))\n",
    "            features.append(np.abs(cv.Scharr(first_order_y, cv.CV_16S, 0, 1)))\n",
    "\n",
    "        return features\n",
    "\n",
    "    def calc_integral_images(self, img: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Calculate integral images.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img : np.ndarray, shape (h, w, c)\n",
    "            uint8 RGB image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        P : np.ndarray, shape (h+1, w+1, n_features)\n",
    "            First order integral images of features.\n",
    "\n",
    "        Q : np.ndarray, shape (h+1, w+1, n_features, n_features)\n",
    "            Second order integral images of features.\n",
    "        \"\"\"\n",
    "        h, w = img.shape[:2]\n",
    "        features = self.extract_features(img)\n",
    "        length = len(features)\n",
    "\n",
    "        # first order integral images\n",
    "        P = cv.integral(np.array(features).transpose((1, 2, 0)))\n",
    "\n",
    "        # second order integral images\n",
    "        Q = cv.integral(\n",
    "            np.array(list(map(lambda x: x[0] * x[1], product(features, features)))).transpose((1, 2, 0))\n",
    "        )\n",
    "        Q = Q.reshape(h + 1, w + 1, length, length)\n",
    "        return P, Q\n",
    "\n",
    "    def calc_covariance(self, P: np.ndarray, Q: np.ndarray, pt1: Tuple[int, int], pt2: Tuple[int, int]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate covariance matrix from integral images.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        P : np.ndarray, shape (h+1, w+1, n_features)\n",
    "            First order integral images of features.\n",
    "\n",
    "        Q : np.ndarray, shape (h+1, w+1, n_features, n_features)\n",
    "            Second order integral images of features.\n",
    "\n",
    "        pt1 : (int, int)\n",
    "            Left top coordinate.\n",
    "\n",
    "        pt2 : (int, int)\n",
    "            Right bottom coordinate.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        covariance : np.ndarray, shape (n_features, n_features)\n",
    "            Covariance matrix.\n",
    "        \"\"\"\n",
    "        x1, y1 = pt1\n",
    "        x2, y2 = pt2\n",
    "        q = Q[y2, x2] + Q[y1, x1] - Q[y1, x2] - Q[y2, x1]\n",
    "        p = P[y2, x2] + P[y1, x1] - P[y1, x2] - P[y2, x1]\n",
    "        n = (y2 - y1) * (x2 - x1)\n",
    "        covariance = np.abs((q - np.outer(p, p) / n) / (n - 1)) + (self.eps * np.identity(P.shape[2]))\n",
    "        return covariance\n",
    "\n",
    "    def set_search_object(self, img: np.ndarray) -> Tuple[np.ndarray, Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Calculate the object covariance matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img : np.ndarray, shape (h, w, c)\n",
    "            uint8 RGB image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        : Fitted detector.\n",
    "        \"\"\"\n",
    "        h, w = img.shape[:2]\n",
    "        P, Q = self.calc_integral_images(img)\n",
    "\n",
    "        # normalize about coordinates\n",
    "        if self.coord:\n",
    "            for i, size in enumerate((w, h)):\n",
    "                P[:, :, i] /= size\n",
    "                Q[:, :, i] /= size\n",
    "                Q[:, :, :, i] /= size\n",
    "\n",
    "        # calculate covariance matrix\n",
    "        obj_cov = self.calc_covariance(P, Q, (0, 0), (w, h))\n",
    "        obj_shape = (h, w)\n",
    "\n",
    "        return obj_cov, obj_shape\n",
    "\n",
    "    def find_object(self, object: Tuple[np.ndarray, Tuple[int, int]], target_img: np.ndarray) -> Tuple[Tuple[int, int], Tuple[int, int], float]:\n",
    "        \"\"\"\n",
    "        Compute object's position in the target image.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        object : (np.ndarray, (h, w))\n",
    "            Object's representative covariance matrix and its height and width \n",
    "\n",
    "        target_img : np.ndarray, shape (h, w, c)\n",
    "            uint8 RGB image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pt1 : (int, int)\n",
    "            Left top coordinate.\n",
    "\n",
    "        pt2 : (int, int)\n",
    "            Right bottom coordinate.\n",
    "\n",
    "        score : float\n",
    "            Dissimilarity of object and target covariance matrices.\n",
    "        \"\"\"\n",
    "        target_h, target_w = target_img.shape[:2]\n",
    "        obj_h, obj_w = object[1]\n",
    "        P, Q = self.calc_integral_images(target_img)\n",
    "\n",
    "        # search window's shape and step size\n",
    "        end = (self.n_scales + 1) // 2\n",
    "        start = end - self.n_scales\n",
    "        shapes = [(int(obj_h * self.ratio ** i), int(obj_w * self.ratio ** i)) for i in range(start, end)]\n",
    "        steps = [int(self.step * self.ratio ** i) for i in range(self.n_scales)]\n",
    "\n",
    "        level_iterations = tqdm(zip(shapes, steps), total=len(shapes))\n",
    "        level_iterations\n",
    "        distances = list()\n",
    "        for level, (shape, step) in enumerate(level_iterations):\n",
    "            level_iterations.set_description(f'Level: {level}, Shape: {shape}, Step: {step}')\n",
    "            p_h, p_w = shape\n",
    "            p_P, p_Q = P.copy(), Q.copy()\n",
    "\n",
    "            # normalize about coordinates\n",
    "            if self.coord:\n",
    "                for i, size in enumerate((p_w, p_h)):\n",
    "                    p_P[:, :, i] /= size\n",
    "                    p_Q[:, :, i] /= size\n",
    "                    p_Q[:, :, :, i] /= size\n",
    "\n",
    "            distance = list()\n",
    "            y1, y2 = 0, p_h\n",
    "            while y2 <= target_h:\n",
    "                dist = list()\n",
    "                x1, x2 = 0, p_w\n",
    "                while x2 <= target_w:\n",
    "                    # calculate covariance matrix\n",
    "                    p_cov = self.calc_covariance(p_P, p_Q, (x1, y1), (x2, y2))\n",
    "\n",
    "                    # jump horizontally\n",
    "                    x1 += step\n",
    "                    x2 += step\n",
    "\n",
    "                    # calculate dissimilarity of two covariance matrices\n",
    "                    dist.append(calc_distance(object[0], p_cov))\n",
    "\n",
    "                # jump vertically\n",
    "                y1 += step\n",
    "                y2 += step\n",
    "                distance.append(dist)\n",
    "            distances.append(np.array(distance))\n",
    "\n",
    "        # choose the most similar window\n",
    "        min_dist_indices = list(map(np.argmin, distances))\n",
    "        min_dist_index = int(np.argmin([dist.flatten()[i] for i, dist in zip(min_dist_indices, distances)]))\n",
    "        min_step = steps[min_dist_index]\n",
    "        min_shape = shapes[min_dist_index]\n",
    "        min_index = min_dist_indices[min_dist_index]\n",
    "        b_h, b_w = distances[min_dist_index].shape\n",
    "\n",
    "        pt1 = ((min_index % b_w) * min_step, (min_index // b_w) * min_step)\n",
    "        pt2 = (pt1[0] + min_shape[1], pt1[1] + min_shape[0])\n",
    "        score = distances[min_dist_index].flatten()[min_index]\n",
    "        \n",
    "        return pt1, pt2, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Algorithm Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectFaces(images, classifier):\n",
    "    \"\"\"\n",
    "    Function for detecting faces\n",
    "\n",
    "    Returns list of rectangles\n",
    "    \"\"\"\n",
    "    detection_iterations = tqdm(images, unit='Image')\n",
    "\n",
    "    face_bboxes = {}\n",
    "    detected_face_visuals = []\n",
    "    for img_idx, frame in enumerate(detection_iterations):\n",
    "        detection_iterations.set_description('Detecting faces')\n",
    "        frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Histogram equalization to improve contrast and make grayscale image more uniform\n",
    "        frame_gray = cv.equalizeHist(frame_gray)\n",
    "\n",
    "        #-- Detect faces\n",
    "        # faces = classifier.detectMultiScale(frame_gray, scaleFactor=1.01, minNeighbors=7, minSize=(175, 175), maxSize=(300, 300), flags=cv.CASCADE_SCALE_IMAGE)\n",
    "        faces = classifier.detectMultiScale(frame_gray, scaleFactor=1.05, minNeighbors=6, minSize=(30, 30), maxSize=(75, 75), flags=cv.CASCADE_SCALE_IMAGE)\n",
    "        visual = frame.copy()\n",
    "        for (x,y,w,h) in faces:\n",
    "            # COLOR IS BGR!\n",
    "            visual = cv.rectangle(visual, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "        face_bboxes[img_idx] = faces\n",
    "        detected_face_visuals.append(visual)\n",
    "\n",
    "    return np.array(detected_face_visuals), face_bboxes\n",
    "\n",
    "def recoverFaceOutlines(images, bboxes, P_mat, pointclouds):\n",
    "    pointcloud_viewer = pptk.viewer(pointclouds[2]['points'], pointclouds[2]['colors']/255)\n",
    "    pointcloud_viewer.set(lookat=np.zeros((3,1)), phi=-np.pi/2, theta=0, point_size=2)\n",
    "    pointcloud_viewer.wait()\n",
    "    pointcloud_viewer.close()\n",
    "\n",
    "def blurFaceOutlines():\n",
    "    # blurLevels = np.ones((1, images.shape[1]))\n",
    "    # images_with_blurred_faces = np.empty(())\n",
    "    # for img_idx, blurLevel in enumerate(blurLevels):\n",
    "    #     images_with_blurred_faces[img_idx] = blurFaces(images_with_faces, face_indices, blurLevel)\n",
    "    pass\n",
    "\n",
    "def generateFacialRecognitionDatabase(path: str, face_detector, face_recognizer: RegionCovarianceDetector) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    image_filenames = sorted(glob(os.path.join(path, '*_frontal.jpg')))\n",
    "    image_iterations = tqdm(image_filenames, unit='Images')\n",
    "    \n",
    "    images = []\n",
    "    for filename in image_iterations:\n",
    "        image_iterations.set_description(f'Loading templating images...')\n",
    "        \n",
    "        raw_img = cv.imread(filename)\n",
    "\n",
    "        img_scale = 512 / raw_img.shape[0]\n",
    "        scaled_width  = int(img_scale * raw_img.shape[1])\n",
    "        scaled_height = int(img_scale * raw_img.shape[0])\n",
    "\n",
    "        images.append(cv.resize(raw_img, (scaled_width, scaled_height), interpolation=cv.INTER_AREA))\n",
    "    \n",
    "    images = np.array(images)\n",
    "    \n",
    "    # Extract face templates to be used for facial recognition\n",
    "    face_templates = []\n",
    "    _, face_bboxes = detectFaces(images, face_detector)\n",
    "    for (img_idx, boxes) in face_bboxes.items():\n",
    "        for (x, y, w, h) in boxes:\n",
    "            face_templates.append(images[img_idx][y:y+h, x:x+w, :])\n",
    "    \n",
    "    template_iterations = tqdm(face_templates, unit='Templates')\n",
    "    template_iterations.set_description('Defining face templates')\n",
    "    database = [face_recognizer.set_search_object(template) for template in template_iterations]\n",
    "\n",
    "    print('Constructed facial recognition database!')\n",
    "    return database\n",
    "\n",
    "def blurRectangularRegion(frame, regions):\n",
    "    for region in regions:\n",
    "        (x, y, w, h) = region\n",
    "        ROI = frame[y:y+h, x:x+w]\n",
    "        blur = cv.GaussianBlur(ROI, (51, 51), 5)\n",
    "        frame[y:y+h, x:x+w] = blur\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6556e2aa584849fbbd5009a80a888015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?Images/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e27113836d4e7683c9cbbafaa7dad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?Image/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26983ad59dc14fdcb1502bdc41b7f2ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?Templates/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed facial recognition database!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d216b3ae8cf4d4c851dbe7980c8f1e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_img_path = 'images/jpg'\n",
    "blurred_img_path = 'images/jpg/blurred'\n",
    "bbox_img_path = 'images/jpg/bounding_boxes'\n",
    "calibration_img_path = 'images/jpg/calibration'\n",
    "pointcloud_path = 'pointclouds'\n",
    "\n",
    "\n",
    "# Make sure output directories are empty\n",
    "cleanDirectories(bbox_img_path, blurred_img_path)\n",
    "\n",
    "# Load in the Facial Detection classifiers\n",
    "face_cascade_alt2 = cv.CascadeClassifier('cascades/haarcascade_frontalface_alt2.xml')\n",
    "\n",
    "# Create Facial Recognizer\n",
    "face_recognizer = RegionCovarianceDetector()\n",
    "\n",
    "database                       = generateFacialRecognitionDatabase(input_img_path, face_cascade_alt2, face_recognizer)\n",
    "images                         = loadImages(input_img_path)\n",
    "pointclouds                    = loadPointClouds(pointcloud_path)\n",
    "K_camera                       = calibrate_camera(calibration_img_path)\n",
    "images_with_faces, face_bboxes = detectFaces(images, face_cascade_alt2)\n",
    "# output_tbd                     = recoverFaceOutlines(images, face_bboxes, None, pointclouds)\n",
    "# blurred_images                 = blurFaceOutlines(images, face_bboxes, output_tbd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1373ee5c41634514af23980f60d0958fc0f2d6819915eababf22cdf09658028c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('ec520_final_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
